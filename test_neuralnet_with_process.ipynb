{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MWScxwiFTmhg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import kagglehub\n",
        "from typing import Dict, List, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mMozhV0uTmhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce289dd2-6c1d-4887-adf4-72a25ff6e79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/parsasam/captcha-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 356M/356M [00:13<00:00, 27.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_path = kagglehub.dataset_download(\"parsasam/captcha-dataset\")\n",
        "dataset_path = Path(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c6caGU6Tmhi",
        "outputId": "c092661a-9930-419b-86c2-cff3080ad636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Label:\n",
            " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "Decoded Label: A1bCd\n"
          ]
        }
      ],
      "source": [
        "CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
        "NUM_CLASSES: int = len(CHARACTERS)\n",
        "CHAR_TO_INDEX = {char: idx for idx, char in enumerate(CHARACTERS)}\n",
        "INDEX_TO_CHAR = {idx: char for char, idx in CHAR_TO_INDEX.items()}\n",
        "CAPTCHA_LENGTH: int = 5  # Captchas are exactly 5 characters long\n",
        "\n",
        "def encode_label(label: str) -> torch.Tensor:\n",
        "    assert len(label) == 5, \"Label must be exactly 5 characters long.\"\n",
        "    indices = [CHAR_TO_INDEX[char] for char in label]\n",
        "    one_hot = torch.zeros(len(label), NUM_CLASSES)\n",
        "    for i, index in enumerate(indices):\n",
        "        one_hot[i][index] = 1\n",
        "    return one_hot\n",
        "\n",
        "def decode_label(one_hot: torch.Tensor) -> str:\n",
        "    decoded_chars = []\n",
        "    for i in range(one_hot.size(0)):\n",
        "        index = torch.argmax(one_hot[i]).item()\n",
        "        decoded_chars.append(INDEX_TO_CHAR[index])\n",
        "    return ''.join(decoded_chars)\n",
        "\n",
        "# Test encoding and decoding\n",
        "label = \"A1bCd\"\n",
        "encoded_label = encode_label(label)\n",
        "print(\"Encoded Label:\\n\", encoded_label)\n",
        "\n",
        "decoded_label = decode_label(encoded_label)\n",
        "print(\"Decoded Label:\", decoded_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hh9EqCbmaTs3"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import cv2\n",
        "\n",
        "# Image processing\n",
        "def process(image):\n",
        "    # 1st part\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    common = []\n",
        "    RANGE=25\n",
        "    for i in range(RANGE):\n",
        "      common.append(Counter(image.flatten()).most_common(RANGE+1)[i][0])\n",
        "\n",
        "\n",
        "    temp = image.copy()\n",
        "    for i, row in enumerate(temp):\n",
        "      for j, pixel in enumerate(row):\n",
        "        if pixel in common:\n",
        "          # temp[i][j] = 0   # keep\n",
        "          pass\n",
        "        else:\n",
        "          temp[i][j] = 255 # discard\n",
        "\n",
        "\n",
        "    ret,temp = cv2.threshold(temp, 0, 255, cv2.THRESH_OTSU)\n",
        "\n",
        "    # limit if too much or too little is filtered out\n",
        "    if Counter(temp.flatten()).most_common(2)[1][1] < 500 or Counter(temp.flatten()).most_common(2)[1][1] > 1000:\n",
        "      ret,image = cv2.threshold(image, 0, 255, cv2.THRESH_OTSU)\n",
        "    else:\n",
        "      image = temp\n",
        "\n",
        "\n",
        "    # 2nd part\n",
        "    if image[0][0] == 0:\n",
        "      image = cv2.bitwise_not(image)\n",
        "\n",
        "    # all non-white pixels are now black\n",
        "    for i, row in enumerate(image):\n",
        "      for j, pixel in enumerate(row):\n",
        "        if pixel != 255:\n",
        "          image[i][j] = 0\n",
        "\n",
        "\n",
        "    # FIXME TODO Try removing GaussianBlur and Dilate when training\n",
        "    image = cv2.GaussianBlur(image, (7,7), 0)\n",
        "\n",
        "    # Creating kernel\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "\n",
        "    image = cv2.dilate(image, kernel, cv2.BORDER_REFLECT)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4lfKHwzBTmhi"
      },
      "outputs": [],
      "source": [
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((40, 150)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class CaptchaDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]  # Assuming .jpg files\n",
        "        self.num_chars = 5  # Fixed length of label (5 characters)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def encode_label(self, label: str) -> torch.Tensor:\n",
        "        \"\"\"Encodes a 5-character label into a one-hot tensor.\"\"\"\n",
        "        assert len(label) == self.num_chars, \"Label must be exactly 5 characters long.\"\n",
        "        indices = [CHAR_TO_INDEX[char] for char in label]\n",
        "        one_hot = torch.zeros(self.num_chars, NUM_CLASSES)\n",
        "        for i, index in enumerate(indices):\n",
        "            one_hot[i][index] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        label = img_name.split('.')[0]  # Extract label from filename (without the .jpg extension)\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Open the image and apply the transform (if any)\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        image = process(image)\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label_one_hot = self.encode_label(label)\n",
        "\n",
        "        return image, label_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0NHTcXiTmhj"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and dataloaders\n",
        "dataset = CaptchaDataset(img_dir=dataset_path, transform=transform)\n",
        "\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(len(dataset) * train_ratio)\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qNQzMj46Tmhj"
      },
      "outputs": [],
      "source": [
        "class CaptchaCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaptchaCNN, self).__init__()\n",
        "\n",
        "        # Define the individual convolutional layers\n",
        "        # self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)    # 1 for black and white\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Define the MaxPool layers\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dummy input to calculate the output size after convolutions\n",
        "        self.dummy_input = torch.zeros(1, 1, 40, 150)  # Example input size: (batch_size, channels, height, width)\n",
        "        conv_output = self._forward_conv(self.dummy_input)  # Get output shape after convolution\n",
        "        conv_output_size = conv_output.view(1, -1).size(1)  # Flatten the output and get size\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(conv_output_size, 256)\n",
        "        self.fc2 = nn.Linear(256, CAPTCHA_LENGTH * NUM_CLASSES)\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        # Pass through the convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through convolution layers\n",
        "        x = self._forward_conv(x)\n",
        "\n",
        "        # Flatten the output of the convolutional layers\n",
        "        x = x.flatten(start_dim=1)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Reshape to match the desired output format\n",
        "        return x.view(-1, CAPTCHA_LENGTH, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6igabocKTmhj"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CaptchaCNN()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oCzVDAs5Tmhj"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model, optimizer, checkpoints_dir=\"checkpoints\"):\n",
        "    # List all checkpoint files in the directory\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoints_dir) if f.endswith(\".pth\")]\n",
        "\n",
        "    # If no checkpoint exists, return the model and optimizer state without modification\n",
        "    if not checkpoint_files:\n",
        "        print(\"No checkpoint found, starting fresh.\")\n",
        "        return model, optimizer, 0, []\n",
        "\n",
        "    # Find the most recent checkpoint (by sorting the files)\n",
        "    latest_checkpoint = max(checkpoint_files, key=lambda f: int(f.split('_')[2].split('.')[0]))  # Sorting by epoch number\n",
        "    checkpoint_path = os.path.join(checkpoints_dir, latest_checkpoint)\n",
        "\n",
        "    # Load checkpoint data\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Restore model state, optimizer state, and other checkpoint data\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Get the starting epoch and loss history\n",
        "    starting_epoch = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss_history']\n",
        "\n",
        "    print(f\"Loaded checkpoint from epoch {starting_epoch}\")\n",
        "\n",
        "    return model, optimizer, starting_epoch, loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbfu7d-VTmhj",
        "outputId": "2111f692-4db0-446c-beb0-4ee30d0265ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from epoch 8\n",
            "Loss history length: 22616\n"
          ]
        }
      ],
      "source": [
        "model, optimizer, start_epoch, loss_history = load_checkpoint(model, optimizer)\n",
        "\n",
        "print(f\"Loss history length: {len(loss_history)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "diP4KUgwTmhk"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, criterion, epochs=10,\n",
        "                print_every=100, checkpoints_dir=\"checkpoints\", starting_epoch=0, loss_history=None):\n",
        "    if loss_history is None:\n",
        "        loss_history = []  # Initialize loss history if not provided\n",
        "\n",
        "    total_batches = len(train_loader)  # Total number of batches per epoch\n",
        "\n",
        "    # Ensure the checkpoints directory exists\n",
        "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(starting_epoch, epochs):  # Start from the checkpoint's epoch\n",
        "        total_loss = 0\n",
        "        start_epoch_time = time.time()  # Record start time of the epoch\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate total elapsed time since the start of training\n",
        "            total_elapsed_time = time.time() - start_epoch_time\n",
        "            avg_batch_time = total_elapsed_time / (batch_idx + 1)\n",
        "\n",
        "            # Print loss and total elapsed time at specified frequency\n",
        "            if batch_idx % print_every == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{total_batches}], Loss: {loss.item():.4f}, \"\n",
        "                      f\"Total Elapsed Time: {total_elapsed_time:.2f}s, Avg Time per Batch: {avg_batch_time:.4f}s\")\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "        # Save checkpoint at the end of the epoch\n",
        "        checkpoint_filename = f\"model_epoch_{epoch+1}.pth\"\n",
        "        checkpoint_path = os.path.join(checkpoints_dir, checkpoint_filename)\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss_history': loss_history,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved model checkpoint for epoch [{epoch+1}/{epochs}] to {checkpoint_filename}\")\n",
        "\n",
        "        # Calculate total epoch time and print the information\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_time = epoch_end_time - start_epoch_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Epoch Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    return loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKUadhNOTmhk",
        "outputId": "69d5cae8-7a70-4539-ff04-c9ac184571c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Batch [1/2827], Loss: 0.0423, Total Elapsed Time: 2.50s, Avg Time per Batch: 2.5023s\n",
            "Epoch [9/20], Batch [101/2827], Loss: 0.0342, Total Elapsed Time: 138.78s, Avg Time per Batch: 1.3740s\n",
            "Epoch [9/20], Batch [201/2827], Loss: 0.0302, Total Elapsed Time: 273.60s, Avg Time per Batch: 1.3612s\n",
            "Epoch [9/20], Batch [301/2827], Loss: 0.0383, Total Elapsed Time: 411.22s, Avg Time per Batch: 1.3662s\n",
            "Epoch [9/20], Batch [401/2827], Loss: 0.0359, Total Elapsed Time: 547.41s, Avg Time per Batch: 1.3651s\n",
            "Epoch [9/20], Batch [501/2827], Loss: 0.0420, Total Elapsed Time: 684.20s, Avg Time per Batch: 1.3657s\n",
            "Epoch [9/20], Batch [601/2827], Loss: 0.0322, Total Elapsed Time: 820.57s, Avg Time per Batch: 1.3653s\n",
            "Epoch [9/20], Batch [701/2827], Loss: 0.0353, Total Elapsed Time: 956.38s, Avg Time per Batch: 1.3643s\n",
            "Epoch [9/20], Batch [801/2827], Loss: 0.0380, Total Elapsed Time: 1095.28s, Avg Time per Batch: 1.3674s\n",
            "Epoch [9/20], Batch [901/2827], Loss: 0.0393, Total Elapsed Time: 1231.68s, Avg Time per Batch: 1.3670s\n",
            "Epoch [9/20], Batch [1001/2827], Loss: 0.0379, Total Elapsed Time: 1365.90s, Avg Time per Batch: 1.3645s\n",
            "Epoch [9/20], Batch [1101/2827], Loss: 0.0394, Total Elapsed Time: 1500.38s, Avg Time per Batch: 1.3627s\n",
            "Epoch [9/20], Batch [1201/2827], Loss: 0.0386, Total Elapsed Time: 1637.57s, Avg Time per Batch: 1.3635s\n",
            "Epoch [9/20], Batch [1301/2827], Loss: 0.0389, Total Elapsed Time: 1773.50s, Avg Time per Batch: 1.3632s\n",
            "Epoch [9/20], Batch [1401/2827], Loss: 0.0404, Total Elapsed Time: 1909.12s, Avg Time per Batch: 1.3627s\n",
            "Epoch [9/20], Batch [1501/2827], Loss: 0.0366, Total Elapsed Time: 2044.67s, Avg Time per Batch: 1.3622s\n",
            "Epoch [9/20], Batch [1601/2827], Loss: 0.0358, Total Elapsed Time: 2180.59s, Avg Time per Batch: 1.3620s\n",
            "Epoch [9/20], Batch [1701/2827], Loss: 0.0402, Total Elapsed Time: 2315.88s, Avg Time per Batch: 1.3615s\n",
            "Epoch [9/20], Batch [1801/2827], Loss: 0.0360, Total Elapsed Time: 2453.06s, Avg Time per Batch: 1.3621s\n",
            "Epoch [9/20], Batch [1901/2827], Loss: 0.0400, Total Elapsed Time: 2588.51s, Avg Time per Batch: 1.3617s\n",
            "Epoch [9/20], Batch [2001/2827], Loss: 0.0380, Total Elapsed Time: 2725.10s, Avg Time per Batch: 1.3619s\n",
            "Epoch [9/20], Batch [2101/2827], Loss: 0.0412, Total Elapsed Time: 2860.16s, Avg Time per Batch: 1.3613s\n",
            "Epoch [9/20], Batch [2201/2827], Loss: 0.0390, Total Elapsed Time: 2996.32s, Avg Time per Batch: 1.3613s\n",
            "Epoch [9/20], Batch [2301/2827], Loss: 0.0337, Total Elapsed Time: 3131.17s, Avg Time per Batch: 1.3608s\n",
            "Epoch [9/20], Batch [2401/2827], Loss: 0.0425, Total Elapsed Time: 3269.43s, Avg Time per Batch: 1.3617s\n",
            "Epoch [9/20], Batch [2501/2827], Loss: 0.0396, Total Elapsed Time: 3405.38s, Avg Time per Batch: 1.3616s\n",
            "Epoch [9/20], Batch [2601/2827], Loss: 0.0390, Total Elapsed Time: 3542.90s, Avg Time per Batch: 1.3621s\n",
            "Epoch [9/20], Batch [2701/2827], Loss: 0.0425, Total Elapsed Time: 3678.31s, Avg Time per Batch: 1.3618s\n",
            "Epoch [9/20], Batch [2801/2827], Loss: 0.0445, Total Elapsed Time: 3814.47s, Avg Time per Batch: 1.3618s\n",
            "Saved model checkpoint for epoch [9/20] to model_epoch_9.pth\n",
            "Epoch 9/20, Epoch Time: 3848.06s\n",
            "Epoch [10/20], Batch [1/2827], Loss: 0.0377, Total Elapsed Time: 2.28s, Avg Time per Batch: 2.2838s\n",
            "Epoch [10/20], Batch [101/2827], Loss: 0.0349, Total Elapsed Time: 136.12s, Avg Time per Batch: 1.3477s\n"
          ]
        }
      ],
      "source": [
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    epochs=20,\n",
        "    print_every=100,\n",
        "    checkpoints_dir='checkpoints',\n",
        "    starting_epoch=start_epoch,\n",
        "    loss_history=loss_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG_psTwbTmhk"
      },
      "outputs": [],
      "source": [
        "# Function to plot training results\n",
        "def plot_results(history):\n",
        "    plt.plot(history, label='Train Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training Loss')\n",
        "    plt.show()\n",
        "\n",
        "plot_results(loss_history[50:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS1ZT4_JTmhk"
      },
      "outputs": [],
      "source": [
        "# Function to predict labels from the model\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(dataloader))\n",
        "\n",
        "    # process step\n",
        "    images = images.to(device)\n",
        "    outputs = model(images)\n",
        "    predicted_labels = []\n",
        "    for output in outputs:\n",
        "        predicted_label = decode_label(output)\n",
        "        predicted_labels.append(predicted_label)\n",
        "    return images.cpu(), predicted_labels\n",
        "\n",
        "# Get some images and their predicted labels\n",
        "images, predicted_labels = predict(model, test_dataloader, device)\n",
        "\n",
        "# Display a few images and their predicted labels\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(images[i+5].permute(1, 2, 0))\n",
        "    ax.set_title(f\"Predicted: {predicted_labels[i+5]}\")\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}